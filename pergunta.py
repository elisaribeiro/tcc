import os
from langchain_chroma import Chroma
from embedding import HuggingFaceEmbedding
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Carrega a chave da API de uma vari√°vel de ambiente por seguran√ßa
client = OpenAI(
    api_key=os.getenv("GITHUB_API_KEY"), # Garanta que GITHUB_API_KEY est√° no seu arquivo .env
    base_url="https://models.github.ai/inference"
)
model_name = "openai/gpt-4o"

embedding = HuggingFaceEmbedding()

vectorstore = Chroma(
    embedding_function=embedding,
    persist_directory="chroma"
)

def perguntar_openai(pergunta, contexto):
    """
    Envia a pergunta e o contexto para o modelo OpenAI e retorna a resposta.
    """
    prompt = (f"Com base nos dados abaixo, responda com exatid√£o √† seguinte pergunta: '{pergunta}'. "
            "Se a pergunta for sobre uma data de evento, procure pela data correspondente √† etapa mencionada e forne√ßa a data diretamente. "
            "Se a data for um per√≠odo, forne√ßa o per√≠odo completo. N√£o adicione informa√ß√µes que n√£o est√£o no contexto.\n\n"
            f"{contexto}\n\nPergunta: {pergunta}") # Repete a pergunta no final para refor√ßar
    
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "Voc√™ √© um assistente √∫til e preciso. Responda apenas com informa√ß√µes contidas no contexto fornecido."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

# Pergunta que ser√° feita ao modelo
pergunta = "Qual √© a data do Lan√ßamento da Chamada P√∫blica FAPESC"

# ESTRAT√âGIA DE RECUPERA√á√ÉO H√çBRIDA:
# Busca documentos de duas formas e os combina para aumentar a chance de incluir o chunk do cronograma.

# Busca 1: Documentos mais similares √† pergunta geral
docs_similar = vectorstore.similarity_search(pergunta, k=15) # Recupera os 15 documentos mais similares

# Busca 2: Documentos espec√≠ficos de cronograma
# Utiliza uma query mais direcionada e filtra pelo tipo de metadado 'cronograma_principal'
docs_cronograma_principal = vectorstore.similarity_search(
    "CRONOGRAMA DE EVENTOS E DATAS IMPORTANTES: " + pergunta,
    k=5, # Pega os 5 mais relevantes desta busca espec√≠fica
    filter={"type": "cronograma_principal"} # Filtra para garantir que seja o chunk do cronograma
)

# Combina e deduplica os documentos recuperados
all_docs = []
seen_content = set() # Usado para evitar adicionar o mesmo chunk mais de uma vez

# Adiciona primeiro os documentos de cronograma principal (dando prioridade a eles)
for doc in docs_cronograma_principal:
    if doc.page_content not in seen_content:
        all_docs.append(doc)
        seen_content.add(doc.page_content)

# Adiciona os documentos gerais, evitando duplicatas
for doc in docs_similar:
    if doc.page_content not in seen_content:
        all_docs.append(doc)
        seen_content.add(doc.page_content)

# A lista 'docs' agora cont√©m os documentos combinados, priorizando os de cronograma
docs = all_docs
print(f"Documentos encontrados (ap√≥s combina√ß√£o e deduplica√ß√£o): {len(docs)}")

# Limite de caracteres para o contexto enviado ao modelo (aproximadamente 8k tokens)
MAX_CHARS = 25000 

contexto = ""
for i, doc in enumerate(docs):
    # Adiciona o chunk ao contexto se houver espa√ßo
    # +2 para o '\n\n' que √© adicionado entre os chunks
    if len(contexto) + len(doc.page_content) + 2 <= MAX_CHARS: 
        contexto += doc.page_content + "\n\n"
        # N√£o √© mais necess√°rio imprimir o status de inclus√£o de cada documento aqui
    else:
        # Se o limite de caracteres for atingido, para de adicionar documentos
        print(f"‚ö†Ô∏è Limite de caracteres do contexto atingido. Pulando documentos restantes.")
        break

print("\nüìå Resposta do modelo:\n")
# Chama a fun√ß√£o para perguntar ao modelo e imprime a resposta
print(perguntar_openai(pergunta, contexto))